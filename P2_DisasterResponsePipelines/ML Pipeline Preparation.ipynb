{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "from IPython import display\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, BaseEstimator, TfidfTransformer\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///InsertDatabaseName.db')\n",
    "df = pd.read_sql(\"SELECT * FROM InsertTableName;\", engine)\n",
    "X = df.message.values\n",
    "Y = df.iloc[:, 4:].values\n",
    "category_label = df.iloc[:, 4:].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['direct',\n",
       "        \"Everybody is gone from the earthquake. My house is crushed. I don't have any possibility to buy anything. If anyone can help me find a job. ..\"],\n",
       "       ['direct',\n",
       "        \"It's not obvious to say the truth the way it is,but this is an emergency and more than emergency.If that's me that you reward,It's will smaller than the one of God.Thank's \"],\n",
       "       ['direct', 'Things are so difficult for me and my child. Thanks'],\n",
       "       ['direct',\n",
       "        \"hello I am a nurse, I don't work, I spend a difficult moment in my life help me I implore of it to you \"],\n",
       "       ['direct',\n",
       "        \"Hello my brother. I ask you to do all that is in your power for us in Avenue Christophe Chanel because we've got problems of water and food, etc. \"],\n",
       "       ['direct', \"what is today's informations? \"],\n",
       "       ['direct',\n",
       "        'finding a little food to eat, because there are alot of us hungry'],\n",
       "       ['direct',\n",
       "        \"My brother and my Mother died from this time there are 6 people in the yard, i am jobless, they give card to take food, they don't give us. \"],\n",
       "       ['direct',\n",
       "        \"Re-start the same if there is one error you don't feel so good.Re- start the same a teason you offend.Re-start the same if the dificults you prevent from to start. \"],\n",
       "       ['direct',\n",
       "        'What do you think about the return of the earthquake '],\n",
       "       ['direct', 'Please help us, we are located at fiveth section. '],\n",
       "       ['direct',\n",
       "        'Good morning, we need some water at village to repatriate center of patmos. '],\n",
       "       ['direct', 'delivering supplies , help serving food'],\n",
       "       ['direct', 'I want to donate a bunch of non-perishable foods .'],\n",
       "       ['direct',\n",
       "        'Im at au Cayes, i would like to know if the local pnud at port au prince has collapsed?? because i have family working there and never got any news from them']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.genre=='direct'].sample(15)[['genre', 'message']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silence_print(true_label, pred_label, silence=True):\n",
    "    \"\"\"Print classification report without warning message\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_label: array\n",
    "        The true values\n",
    "    pred_label: array\n",
    "        The predict values\n",
    "    silence: boolean default True\n",
    "        If true, print the report without warning message\n",
    "    \"\"\"\n",
    "    if silence:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            for index, column in enumerate(category_label):\n",
    "                report = classification_report(\n",
    "                    true_label[:, index], pred_label[:, index], labels=np.arange(1), \\\n",
    "                    target_names=[column], output_dict=True, digits=2\n",
    "                )\n",
    "                print(\"{0:<25}\\n  Accuracy:{1: 5.3f}  Precision:{2: =6.3f}   Recall:{3:< 5.3f}\".format(\n",
    "                    column, accuracy_score(true_label[:, index], pred_label[:, index]), \n",
    "                    report[column][\"precision\"], report[column][\"recall\"], report[column][\"f1-score\"]\n",
    "                ))\n",
    "    else:\n",
    "        for index, column in enumerate(category_label):\n",
    "            report = classification_report(\n",
    "                    true_label[:, index], pred_label[:, index], labels=np.arange(1), \\\n",
    "                    target_names=[column], output_dict=True, digits=2\n",
    "                )\n",
    "            print(\"{0:<25}\\n  Accuracy:{1: 5.3f}  Precision:{2: =6.3f}   Recall:{3:< 5.3f}\".format(\n",
    "                    column, accuracy_score(true_label[:, index], pred_label[:, index]), \n",
    "                    report[column][\"precision\"], report[column][\"recall\"], report[column][\"f1-score\"]\n",
    "                ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenize the text \n",
    "    \n",
    "    Tokenize the text information by using word_tokenize, WordNetLemmatizer, \n",
    "    remove words that is in stopwords\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text: string\n",
    "        String contains the message information\n",
    "    \n",
    "    Results:\n",
    "    ----------\n",
    "    result: list\n",
    "        List contains strings parsed from the text\n",
    "    \"\"\"\n",
    "    # nromalize text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            result.append(lemmatizer.lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "- You'll find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42, n_jobs=4)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"text_pipeline\", Pipeline([\n",
    "        (\"vect\", CountVectorizer(tokenizer=tokenize)),\n",
    "        (\"tfidf\", TfidfTransformer())\n",
    "    ])),\n",
    "    (\"clf\", MultiOutputClassifier(forest, n_jobs=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "* Split data into train and test sets\n",
    "* Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('text_pipeline', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1...,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "           n_jobs=2))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data\n",
    "pipeline.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic model\n",
    "pred_y = pipeline.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related                  \n",
      "  Accuracy: 0.792  Precision: 0.601   Recall: 0.451\n",
      "request                  \n",
      "  Accuracy: 0.885  Precision: 0.897   Recall: 0.973\n",
      "offer                    \n",
      "  Accuracy: 0.995  Precision: 0.995   Recall: 1.000\n",
      "aid_related              \n",
      "  Accuracy: 0.752  Precision: 0.762   Recall: 0.842\n",
      "medical_help             \n",
      "  Accuracy: 0.922  Precision: 0.926   Recall: 0.994\n",
      "medical_products         \n",
      "  Accuracy: 0.951  Precision: 0.952   Recall: 0.999\n",
      "search_and_rescue        \n",
      "  Accuracy: 0.976  Precision: 0.976   Recall: 1.000\n",
      "security                 \n",
      "  Accuracy: 0.981  Precision: 0.982   Recall: 1.000\n",
      "military                 \n",
      "  Accuracy: 0.970  Precision: 0.973   Recall: 0.997\n",
      "child_alone              \n",
      "  Accuracy: 1.000  Precision: 1.000   Recall: 1.000\n",
      "water                    \n",
      "  Accuracy: 0.953  Precision: 0.955   Recall: 0.996\n",
      "food                     \n",
      "  Accuracy: 0.936  Precision: 0.943   Recall: 0.988\n",
      "shelter                  \n",
      "  Accuracy: 0.929  Precision: 0.935   Recall: 0.991\n",
      "clothing                 \n",
      "  Accuracy: 0.986  Precision: 0.986   Recall: 1.000\n",
      "money                    \n",
      "  Accuracy: 0.979  Precision: 0.979   Recall: 1.000\n",
      "missing_people           \n",
      "  Accuracy: 0.988  Precision: 0.988   Recall: 1.000\n",
      "refugees                 \n",
      "  Accuracy: 0.968  Precision: 0.970   Recall: 0.998\n",
      "death                    \n",
      "  Accuracy: 0.957  Precision: 0.959   Recall: 0.998\n",
      "other_aid                \n",
      "  Accuracy: 0.870  Precision: 0.875   Recall: 0.992\n",
      "infrastructure_related   \n",
      "  Accuracy: 0.935  Precision: 0.936   Recall: 0.998\n",
      "transport                \n",
      "  Accuracy: 0.956  Precision: 0.957   Recall: 0.999\n",
      "buildings                \n",
      "  Accuracy: 0.954  Precision: 0.955   Recall: 0.999\n",
      "electricity              \n",
      "  Accuracy: 0.980  Precision: 0.980   Recall: 1.000\n",
      "tools                    \n",
      "  Accuracy: 0.994  Precision: 0.994   Recall: 1.000\n",
      "hospitals                \n",
      "  Accuracy: 0.990  Precision: 0.990   Recall: 1.000\n",
      "shops                    \n",
      "  Accuracy: 0.996  Precision: 0.996   Recall: 1.000\n",
      "aid_centers              \n",
      "  Accuracy: 0.987  Precision: 0.987   Recall: 1.000\n",
      "other_infrastructure     \n",
      "  Accuracy: 0.956  Precision: 0.957   Recall: 0.999\n",
      "weather_related          \n",
      "  Accuracy: 0.860  Precision: 0.866   Recall: 0.955\n",
      "floods                   \n",
      "  Accuracy: 0.947  Precision: 0.951   Recall: 0.994\n",
      "storm                    \n",
      "  Accuracy: 0.939  Precision: 0.951   Recall: 0.983\n",
      "fire                     \n",
      "  Accuracy: 0.990  Precision: 0.990   Recall: 1.000\n",
      "earthquake               \n",
      "  Accuracy: 0.967  Precision: 0.975   Recall: 0.989\n",
      "cold                     \n",
      "  Accuracy: 0.979  Precision: 0.980   Recall: 1.000\n",
      "other_weather            \n",
      "  Accuracy: 0.948  Precision: 0.950   Recall: 0.998\n",
      "direct_report            \n",
      "  Accuracy: 0.843  Precision: 0.854   Recall: 0.970\n"
     ]
    }
   ],
   "source": [
    "# print the report\n",
    "silence_print(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"text_pipeline\", Pipeline([\n",
    "        (\"vect\", CountVectorizer(tokenizer=tokenize)),\n",
    "        (\"tfidf\", TfidfTransformer())\n",
    "    ])),\n",
    "    (\"clf\", MultiOutputClassifier(forest))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "params = {\n",
    "#     'text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'text_pipeline__vect__max_df': (0.75, 1.0),\n",
    "    'text_pipeline__vect__max_features': (5000, 100000),\n",
    "#     'text_pipeline__tfidf__use_idf': (True, False),\n",
    "    'clf__estimator__n_estimators': [10, 20, 30],\n",
    "    'clf__estimator__criterion': ['gini', 'entropy'],\n",
    "    'clf__estimator__max_depth': [4, 6, 10],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('text_pipeline', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1...           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "           n_jobs=None))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'text_pipeline__vect__max_features': (5000, 100000), 'clf__estimator__n_estimators': [10, 20, 30], 'clf__estimator__criterion': ['gini', 'entropy'], 'clf__estimator__max_depth': [4, 6, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "cv = GridSearchCV(pipeline, param_grid=params, cv=3)\n",
    "\n",
    "cv.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values\n",
    "pred_y = cv.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related                  \n",
      "  Accuracy: 0.754  Precision: 0.000   Recall: 0.000\n",
      "request                  \n",
      "  Accuracy: 0.831  Precision: 0.831   Recall: 1.000\n",
      "offer                    \n",
      "  Accuracy: 0.995  Precision: 0.995   Recall: 1.000\n",
      "aid_related              \n",
      "  Accuracy: 0.598  Precision: 0.595   Recall: 1.000\n",
      "medical_help             \n",
      "  Accuracy: 0.919  Precision: 0.919   Recall: 1.000\n",
      "medical_products         \n",
      "  Accuracy: 0.947  Precision: 0.947   Recall: 1.000\n",
      "search_and_rescue        \n",
      "  Accuracy: 0.976  Precision: 0.976   Recall: 1.000\n",
      "security                 \n",
      "  Accuracy: 0.982  Precision: 0.982   Recall: 1.000\n",
      "military                 \n",
      "  Accuracy: 0.969  Precision: 0.969   Recall: 1.000\n",
      "child_alone              \n",
      "  Accuracy: 1.000  Precision: 1.000   Recall: 1.000\n",
      "water                    \n",
      "  Accuracy: 0.936  Precision: 0.936   Recall: 1.000\n",
      "food                     \n",
      "  Accuracy: 0.888  Precision: 0.888   Recall: 1.000\n",
      "shelter                  \n",
      "  Accuracy: 0.910  Precision: 0.910   Recall: 1.000\n",
      "clothing                 \n",
      "  Accuracy: 0.985  Precision: 0.985   Recall: 1.000\n",
      "money                    \n",
      "  Accuracy: 0.978  Precision: 0.978   Recall: 1.000\n",
      "missing_people           \n",
      "  Accuracy: 0.988  Precision: 0.988   Recall: 1.000\n",
      "refugees                 \n",
      "  Accuracy: 0.967  Precision: 0.967   Recall: 1.000\n",
      "death                    \n",
      "  Accuracy: 0.953  Precision: 0.953   Recall: 1.000\n",
      "other_aid                \n",
      "  Accuracy: 0.869  Precision: 0.869   Recall: 1.000\n",
      "infrastructure_related   \n",
      "  Accuracy: 0.936  Precision: 0.936   Recall: 1.000\n",
      "transport                \n",
      "  Accuracy: 0.954  Precision: 0.954   Recall: 1.000\n",
      "buildings                \n",
      "  Accuracy: 0.950  Precision: 0.950   Recall: 1.000\n",
      "electricity              \n",
      "  Accuracy: 0.979  Precision: 0.979   Recall: 1.000\n",
      "tools                    \n",
      "  Accuracy: 0.994  Precision: 0.994   Recall: 1.000\n",
      "hospitals                \n",
      "  Accuracy: 0.990  Precision: 0.990   Recall: 1.000\n",
      "shops                    \n",
      "  Accuracy: 0.996  Precision: 0.996   Recall: 1.000\n",
      "aid_centers              \n",
      "  Accuracy: 0.987  Precision: 0.987   Recall: 1.000\n",
      "other_infrastructure     \n",
      "  Accuracy: 0.957  Precision: 0.957   Recall: 1.000\n",
      "weather_related          \n",
      "  Accuracy: 0.726  Precision: 0.726   Recall: 1.000\n",
      "floods                   \n",
      "  Accuracy: 0.921  Precision: 0.921   Recall: 1.000\n",
      "storm                    \n",
      "  Accuracy: 0.906  Precision: 0.906   Recall: 1.000\n",
      "fire                     \n",
      "  Accuracy: 0.989  Precision: 0.989   Recall: 1.000\n",
      "earthquake               \n",
      "  Accuracy: 0.911  Precision: 0.911   Recall: 1.000\n",
      "cold                     \n",
      "  Accuracy: 0.978  Precision: 0.978   Recall: 1.000\n",
      "other_weather            \n",
      "  Accuracy: 0.947  Precision: 0.947   Recall: 1.000\n",
      "direct_report            \n",
      "  Accuracy: 0.804  Precision: 0.804   Recall: 1.000\n"
     ]
    }
   ],
   "source": [
    "# print report\n",
    "silence_print(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunck_tokenize(text):\n",
    "    \"\"\"Tokenize the text \n",
    "    \n",
    "    Tokenize the text information by using word_tokenize, WordNetLemmatizer, \n",
    "    remove words that is in stopwords, and use the new regular expresssiong\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text: string\n",
    "        String contains the message information\n",
    "    \n",
    "    Results:\n",
    "    ----------\n",
    "    result: list\n",
    "        List contains strings parsed from the text\n",
    "    \"\"\"\n",
    "    # nromalize text\n",
    "    text = re.sub(r\"#[a-zA-Z0-9]\", \"tagname\", text)\n",
    "    text = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"urlholder\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize and remove stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token.lower() not in stop_words:\n",
    "            result.append(lemmatizer.lemmatize(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(x):\n",
    "    \"\"\"Parse the text into more information\n",
    "    Get the more information to analyze, like sentence length over 1\n",
    "    and check the pos tag type\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: string\n",
    "        Messages string\n",
    "    \n",
    "    Results:\n",
    "    ----------\n",
    "    length: boolean\n",
    "        Check the sentence length is over 1\n",
    "    chunck: boolean\n",
    "        Check whether there is specific pos tag\n",
    "    \"\"\"\n",
    "    length = len(sent_tokenize(x))\n",
    "    chunck = False\n",
    "\n",
    "    for sentence in sent_tokenize(x):\n",
    "        pos_tags = nltk.pos_tag(chunck_tokenize(sentence))\n",
    "\n",
    "        try:\n",
    "            word, tag = pos_tags[0]\n",
    "\n",
    "            if \"VB\" in tag:\n",
    "                chunck = True\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            for word, tag in nltk.pos_tag(chunck_tokenize(sentence))[:3]:\n",
    "                if tag == \"NNP\":\n",
    "                    chunck = True\n",
    "                    break\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        if chunck:\n",
    "            break\n",
    "\n",
    "    return (length, chunck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordChunckExtract(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Word Pos Tag Extract\n",
    "    \n",
    "    Word Chunck extract the text information, which is used to \n",
    "    fit and transform method\n",
    "    \"\"\"\n",
    "    def starting_verb(self, text):\n",
    "        \"\"\"Parse the text into more information\n",
    "        Get the more information to analyze, like sentence length over 1\n",
    "        and check the pos tag type\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text: string\n",
    "            Messages string\n",
    "\n",
    "        Results:\n",
    "        ----------\n",
    "        length: boolean\n",
    "            Check the sentence length is over 1\n",
    "        chunck: boolean\n",
    "            Check whether there is specific pos tag\n",
    "        \"\"\"\n",
    "        length = len(sent_tokenize(text))\n",
    "        chunck = False\n",
    "\n",
    "        for sentence in sent_tokenize(text):\n",
    "            pos_tags = nltk.pos_tag(chunck_tokenize(sentence))\n",
    "\n",
    "            try:\n",
    "                word, tag = pos_tags[0]\n",
    "\n",
    "                if \"VB\" in tag:\n",
    "                    chunck = True\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                for word, tag in nltk.pos_tag(chunck_tokenize(sentence))[:3]:\n",
    "                    if tag == \"NNP\":\n",
    "                        chunck = True\n",
    "                        break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if chunck:\n",
    "                break\n",
    "\n",
    "        return (length, chunck)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        \"\"\"\n",
    "        Fit the text message\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the text message\n",
    "        \"\"\"\n",
    "        result = pd.DataFrame()\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        result[\"length\"] = X_tagged.apply(lambda x: x[0] > 1)\n",
    "        result[\"chucnk\"] = X_tagged.apply(lambda x: x[1])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42, n_jobs=4)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"features\", FeatureUnion([\n",
    "        (\"text_pipeline\", Pipeline([\n",
    "            (\"vect\", CountVectorizer(tokenizer=tokenize)),\n",
    "            (\"tfidf\", TfidfTransformer())\n",
    "        ])), \n",
    "        (\"exact_chunck\", WordChunckExtract())\n",
    "    ])),\n",
    "    (\"clf\", MultiOutputClassifier(forest, n_jobs=4))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('text_pipeline', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, ma...,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "           n_jobs=4))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'features__text_pipeline__vect__max_features': (None, 5000, 10000), 'clf__estimator__n_estimators': [100, 150], 'clf__estimator__criterion': ['gini', 'entropy'], 'clf__estimator__max_depth': [12, 18, 25], 'features__transformer_weights': ({'text_pipeline': 1, 'starting_verb': 0.5},)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params\n",
    "params = {\n",
    "#     'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'features__text_pipeline__vect__max_df': (0.75, 1.0),\n",
    "    'features__text_pipeline__vect__max_features': (None, 5000, 10000),\n",
    "#     'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "    'clf__estimator__n_estimators': [100, 150],\n",
    "    'clf__estimator__criterion': ['gini', 'entropy'],\n",
    "    'clf__estimator__max_depth': [12, 18, 25],\n",
    "    'features__transformer_weights': (\n",
    "    {'text_pipeline': 1, 'starting_verb': 0.5},\n",
    "#     {'text_pipeline': 0.5, 'starting_verb': 1},\n",
    "#     {'text_pipeline': 0.8, 'starting_verb': 1},\n",
    ")\n",
    "\n",
    "}\n",
    "\n",
    "# train model\n",
    "cv = GridSearchCV(pipeline, param_grid=params, cv=3)\n",
    "\n",
    "cv.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values\n",
    "pred_y = cv.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related                  \n",
      "  Accuracy: 0.754  Precision: 0.000   Recall: 0.000\n",
      "request                  \n",
      "  Accuracy: 0.831  Precision: 0.831   Recall: 1.000\n",
      "offer                    \n",
      "  Accuracy: 0.995  Precision: 0.995   Recall: 1.000\n",
      "aid_related              \n",
      "  Accuracy: 0.633  Precision: 0.618   Recall: 0.994\n",
      "medical_help             \n",
      "  Accuracy: 0.919  Precision: 0.919   Recall: 1.000\n",
      "medical_products         \n",
      "  Accuracy: 0.947  Precision: 0.947   Recall: 1.000\n",
      "search_and_rescue        \n",
      "  Accuracy: 0.976  Precision: 0.976   Recall: 1.000\n",
      "security                 \n",
      "  Accuracy: 0.982  Precision: 0.982   Recall: 1.000\n",
      "military                 \n",
      "  Accuracy: 0.969  Precision: 0.969   Recall: 1.000\n",
      "child_alone              \n",
      "  Accuracy: 1.000  Precision: 1.000   Recall: 1.000\n",
      "water                    \n",
      "  Accuracy: 0.936  Precision: 0.936   Recall: 1.000\n",
      "food                     \n",
      "  Accuracy: 0.888  Precision: 0.888   Recall: 1.000\n",
      "shelter                  \n",
      "  Accuracy: 0.910  Precision: 0.910   Recall: 1.000\n",
      "clothing                 \n",
      "  Accuracy: 0.985  Precision: 0.985   Recall: 1.000\n",
      "money                    \n",
      "  Accuracy: 0.978  Precision: 0.978   Recall: 1.000\n",
      "missing_people           \n",
      "  Accuracy: 0.988  Precision: 0.988   Recall: 1.000\n",
      "refugees                 \n",
      "  Accuracy: 0.967  Precision: 0.967   Recall: 1.000\n",
      "death                    \n",
      "  Accuracy: 0.953  Precision: 0.953   Recall: 1.000\n",
      "other_aid                \n",
      "  Accuracy: 0.869  Precision: 0.869   Recall: 1.000\n",
      "infrastructure_related   \n",
      "  Accuracy: 0.936  Precision: 0.936   Recall: 1.000\n",
      "transport                \n",
      "  Accuracy: 0.954  Precision: 0.954   Recall: 1.000\n",
      "buildings                \n",
      "  Accuracy: 0.950  Precision: 0.950   Recall: 1.000\n",
      "electricity              \n",
      "  Accuracy: 0.979  Precision: 0.979   Recall: 1.000\n",
      "tools                    \n",
      "  Accuracy: 0.994  Precision: 0.994   Recall: 1.000\n",
      "hospitals                \n",
      "  Accuracy: 0.990  Precision: 0.990   Recall: 1.000\n",
      "shops                    \n",
      "  Accuracy: 0.996  Precision: 0.996   Recall: 1.000\n",
      "aid_centers              \n",
      "  Accuracy: 0.987  Precision: 0.987   Recall: 1.000\n",
      "other_infrastructure     \n",
      "  Accuracy: 0.957  Precision: 0.957   Recall: 1.000\n",
      "weather_related          \n",
      "  Accuracy: 0.730  Precision: 0.729   Recall: 1.000\n",
      "floods                   \n",
      "  Accuracy: 0.921  Precision: 0.921   Recall: 1.000\n",
      "storm                    \n",
      "  Accuracy: 0.906  Precision: 0.906   Recall: 1.000\n",
      "fire                     \n",
      "  Accuracy: 0.989  Precision: 0.989   Recall: 1.000\n",
      "earthquake               \n",
      "  Accuracy: 0.911  Precision: 0.911   Recall: 1.000\n",
      "cold                     \n",
      "  Accuracy: 0.978  Precision: 0.978   Recall: 1.000\n",
      "other_weather            \n",
      "  Accuracy: 0.947  Precision: 0.947   Recall: 1.000\n",
      "direct_report            \n",
      "  Accuracy: 0.804  Precision: 0.804   Recall: 1.000\n"
     ]
    }
   ],
   "source": [
    "silence_print(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.pickle\", \"wb\") as file:\n",
    "    pickle.dump(cv, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
